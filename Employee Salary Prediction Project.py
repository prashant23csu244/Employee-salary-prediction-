# -*- coding: utf-8 -*-
"""Employee_Salary_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CWr_wkGwizO07YHPZdA7zQMouTmiuo5z
"""

import pandas as pd

data=pd.read_csv('/content/synthetic_adult_dataset.csv')

data.head(10)

data.tail(3)

data.shape

data.isna().sum()

print(data.occupation.value_counts())

print(data.workclass.value_counts())

data.workclass.replace({'?':'Others'},inplace=True)
print(data['workclass'].value_counts())

data.occupation.replace({'?':'Others'},inplace=True)
print(data['occupation'].value_counts())

data=data[data['workclass']!='Without-pay']
data=data[data['workclass']!='Never-worked']
print(data['workclass'].value_counts())

print(data.relationship.value_counts())

import matplotlib.pyplot as plt   #visualization
plt.boxplot(data['age'])
plt.show()

data=data[(data['age']<=75)&(data['age']>=17)]

plt.boxplot(data['age'])
plt.show()

data.shape

plt.boxplot(data['capital-gain'])
plt.show()

plt.boxplot(data['educational-num'])
plt.show()

data=data[(data['educational-num']<=16)&(data['educational-num']>=5)]

plt.boxplot(data['educational-num'])
plt.show()

plt.boxplot(data['educational-num'])
plt.show()



data.shape

data=data.drop(columns=['education']) #redundant features removal

data

from sklearn.preprocessing import LabelEncoder   #import libarary
encoder=LabelEncoder()                       #create object
data['workclass']=encoder.fit_transform(data['workclass']) #7 categories   0,1, 2, 3, 4, 5, 6,
data['marital-status']=encoder.fit_transform(data['marital-status'])   #3 categories 0, 1, 2
data['occupation']=encoder.fit_transform(data['occupation'])
data['relationship']=encoder.fit_transform(data['relationship'])      #5 categories  0, 1, 2, 3, 4
data['race']=encoder.fit_transform(data['race'])
data['gender']=encoder.fit_transform(data['gender'])    #2 catogories     0, 1
data['native-country']=encoder.fit_transform(data['native-country'])

data

x=data.drop(columns=['income'])
y=data['income']
x

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
x=scaler.fit_transform(x)
x

from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2,random_state=23,stratify=y)

xtrain

from sklearn.neighbors import KNeighborsClassifier
knn=KNeighborsClassifier()
knn.fit(xtrain,ytrain)
knn.fit(xtrain,ytrain)
predict =knn.predict(xtest)
predict

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict)

from sklearn.linear_model import LogisticRegression
ln=LogisticRegression()
ln.fit(xtrain,ytrain)
ln.fit(xtrain,ytrain)
predict1 =ln.predict(xtest)
predict1

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict1)

from sklearn.neural_network import MLPClassifier
clf=MLPClassifier(solver='adam',hidden_layer_sizes=(5,2),random_state=2,max_iter=2000)
clf.fit(xtrain,ytrain)

predict2 =clf.predict(xtest)
predict2

from sklearn.metrics import accuracy_score
accuracy_score(ytest,predict2)

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Load dataset
df = pd.read_csv("/content/synthetic_adult_dataset.csv")  # Update path if needed

# Features and target
X = df.drop("income", axis=1)
y = df["income"]

# Categorical and numerical columns
# Based on the initial data exploration, 'educational-num' and 'hours-per-week' are numerical.
# 'education' and 'occupation' are categorical.
cat_cols = ['education', 'occupation']
num_cols = ['age', 'hours-per-week', 'educational-num', 'fnlwgt', 'capital-gain', 'capital-loss']


# Preprocessing pipeline
preprocessor = ColumnTransformer([
    ("num", StandardScaler(), num_cols),
    ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols)
])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)

# Define models
base_models = {
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "RandomForest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(),
    "GradientBoosting": GradientBoostingClassifier()
}

# Train and evaluate each model
results = {}
for name, model in base_models.items():
    pipe = Pipeline([
        ("preprocessor", preprocessor),
        ("classifier", model)
    ])
    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)
    acc = accuracy_score(y_test, preds)
    results[name] = {"accuracy": acc, "pipeline": pipe}
    print(f"{name}: {acc:.4f}")

import joblib

# Plot accuracy scores
model_names = results.keys()
accuracy_scores = [results[name]["accuracy"] for name in model_names]

plt.figure(figsize=(10, 6))
plt.bar(model_names, accuracy_scores, color='skyblue')
plt.ylabel("Accuracy Score")
plt.title("Model Comparison")
plt.xticks(rotation=45)
plt.grid(axis="y")
plt.tight_layout()
plt.show()

# Find and save best model pipeline
best_model_name = max(results, key=lambda x: results[x]["accuracy"])
best_pipeline = results[best_model_name]["pipeline"]
print(f"\nBest model: {best_model_name} with accuracy {results[best_model_name]['accuracy']:.4f}")

joblib.dump(best_pipeline, "best_model.pkl")
print("‚úÖ Saved best model pipeline as best_model.pkl")

pip install streamlit





 Commented out IPython magic to ensure Python compatibility.
 %%writefile app.py
 import streamlit as st
 import pandas as pd
 import joblib
 
 # Load the trained model
 model = joblib.load("best_model.pkl")
 
 st.set_page_config(page_title="Employee Salary Classification", page_icon="üíº", layout="centered")
 
 st.title("üíº Employee Salary Classification App")
 st.markdown("Predict whether an employee earns >50K or ‚â§50K based on input features.")
 
 # Sidebar inputs (these must match your training feature columns)
 st.sidebar.header("Input Employee Details")
 
 # ‚ú® Replace these fields with your dataset's actual input columns
 age = st.sidebar.slider("Age", 18, 65, 30)
 education = st.sidebar.selectbox("Education Level", [
     "Bachelors", "Masters", "PhD", "HS-grad", "Assoc", "Some-college"
 ])
 occupation = st.sidebar.selectbox("Job Role", [
     "Tech-support", "Craft-repair", "Other-service", "Sales",
     "Exec-managerial", "Prof-specialty", "Handlers-cleaners", "Machine-op-inspct",
     "Adm-clerical", "Farming-fishing", "Transport-moving", "Priv-house-serv",
     "Protective-serv", "Armed-Forces"
 ])
 hours_per_week = st.sidebar.slider("Hours per week", 1, 80, 40)
 experience = st.sidebar.slider("Years of Experience", 0, 40, 5)
 
 # Build input DataFrame (‚ö†Ô∏è must match preprocessing of your training data)
 input_df = pd.DataFrame({
     'age': [age],
     'workclass': ["Private"],  # Default workclass
     'fnlwgt': [200000],        # Default fnlwgt
     'education': [education],
     'educational-num': [10],   # Approximate for Bachelors
     'marital-status': ["Never-married"],
     'occupation': [occupation],
     'relationship': ["Not-in-family"],
     'race': ["White"],
     'gender': ["Male"],
     'capital-gain': [0],
     'capital-loss': [0],
     'hours-per-week': [hours_per_week],
     'native-country': ["United-States"]
 })
 
 
 st.write("### üîé Input Data")
 st.write(input_df)
 
 # Predict button
 if st.button("Predict Salary Class"):
     prediction = model.predict(input_df)
     st.success(f"‚úÖ Prediction: {prediction[0]}")
 
 # Batch prediction
 st.markdown("---")
 st.markdown("#### üìÇ Batch Prediction")
 uploaded_file = st.file_uploader("Upload a CSV file for batch prediction", type="csv")
 if uploaded_file is not None:
     batch_data = pd.read_csv(uploaded_file)
 
     # Fill missing required columns with default values
     required_columns = {
         'fnlwgt': 200000,
         'educational-num': 10,
         'capital-gain': 0,
         'capital-loss': 0,
         'hours-per-week': 40,
         'occupation': 'Other-service'
     }
 
     for col, default_val in required_columns.items():
         if col not in batch_data.columns:
             batch_data[col] = default_val
 
     st.write("Uploaded data preview:", batch_data.head())
 
     # Predict
     batch_preds = model.predict(batch_data)
     batch_data['PredictedClass'] = batch_preds
 
     st.write("‚úÖ Predictions:")
     st.write(batch_data.head())
 
     csv = batch_data.to_csv(index=False).encode('utf-8')
    st.download_button("Download Predictions CSV", csv, file_name='synthetic_adult_dataset.csv', mime='text/csv')
 


pip install streamlit pyngrok

!ngrok http 8501

! ngrok authtoken 302WAmvtGIQggHHjSieUiBxBxM9_4TbW2EdSkEvxWPKu163Fq

import os
import threading
def run_streamlit():
    os.system('streamlit run app.py-server.port 8501')
thread = threading.Thread(target=run_streamlit)
thread.start()

from pyngrok import ngrok
import threading
import os

ngrok.kill()  # kill old tunnels
public_url = ngrok.connect(8501)
print("üîó OPEN THIS LINK in your browser:", public_url)

def run_app():
    os.system("streamlit run app.py")

threading.Thread(target=run_app).start()
